# AWS Deployment Configuration
# Copy to terraform.tfvars and customize

# REQUIRED PARAMETERS
# -------------------------

# SSH key for EC2 access (create in AWS Console or CLI)
# ssh_key_name = "your-key-name"

# AWS region
# aws_region = "us-east-1"

# Environment name
# environment = "production"

# EC2 instance type
# instance_type = "t3.medium"

# EBS volume size for data (GiB)
# data_volume_size = 50

# Storage types
# data_volume_type = "gp3"  # gp3 (SSD), st1 (HDD throughput optimized, min 125 GiB), sc1 (HDD cold, min 125 GiB)
# root_volume_type = "gp3"  # gp3 (SSD), gp2 (older SSD)

# Grafana admin password
# grafana_password = "YourSecurePassword123!"

# CIDR blocks for SSH access
# Get your IP: curl ifconfig.me
# allowed_ssh_cidr = [
#   "0.0.0.0/0"       # WARNING: Allows access from anywhere - NOT RECOMMENDED
#   # "1.2.3.4/32"    # RECOMMENDED: Replace with YOUR actual IP address
# ]

# CIDR blocks for Grafana access
# IMPORTANT: Empty list [] = NO direct access (SSH tunnel required, most secure)
# allowed_cidr_blocks = []            # No direct access, SSH tunnel only (recommended)
# allowed_cidr_blocks = ["1.2.3.4/32"]  # Replace with YOUR actual IP for direct access
# allowed_cidr_blocks = ["0.0.0.0/0"]   # WARNING: Allows access from anywhere - NOT RECOMMENDED

# Allocate Elastic IP for stable address
# use_elastic_ip = true

# Port binding configuration (optional, defaults to most secure)
# bind_host = "127.0.0.1:"  # Bind internal services to localhost only (default, most secure)
# bind_host = ""            # OR: Bind to all interfaces (needed if accessing services remotely)

# grafana_bind_host = "127.0.0.1:" # Grafana only via SSH tunnel (default, most secure)
# grafana_bind_host = ""           # OR: Grafana accessible from outside (controlled by Security Group)



# OPTIONAL PARAMETERS
# -------------------------

# postgres_ai version (optional, defaults to actual)
# postgres_ai_version = "0.10" # branch or specific tag like "0.10"

# PostgreSQL instances to monitor (optional, can be empty for initial setup)
# monitoring_instances = [
#   {
#     name        = "production-db"
#     conn_str    = "postgresql://monitor:password@db.example.com:5432/postgres"
#     environment = "production"
#     cluster     = "main"
#     node_name   = "primary"
#   },
#   {
#     name        = "production-replica"
#     conn_str    = "postgresql://monitor:password@replica.example.com:5432/postgres"
#     environment = "production"
#     cluster     = "main"
#     node_name   = "replica-1"
#   }
# ]

# PostgresAI API key (optional, for uploading reports to cloud)
# 
# How to get API key:
# 1. Register at https://console.postgres.ai
# 2. Go to: Your Organization → Manage → Access Tokens
# 3. Create new token and copy it here
#
# If not set, reports will be generated locally without upload
# postgres_ai_api_key = "your-api-key-here"

# Enable demo database (optional, for testing)
# enable_demo_db = false


# CONFIGURATION EXAMPLES
# -------------------------

# Testing with demo database:
# ----------------------------
# Leave monitoring_instances commented out
# Set: enable_demo_db = true (optional parameter)

# Production with multiple databases:
# ------------------------------------
# Uncomment and fill monitoring_instances with your databases
# Set: instance_type = "t3.xlarge" for better performance
# Set: data_volume_size = 200 for more storage
# Restrict: allowed_ssh_cidr and allowed_cidr_blocks to your network

# Development/staging:
# --------------------
# Set: instance_type = "t3.small" (may be slow)
# Set: data_volume_size = 30 (minimal storage)
# Set: use_elastic_ip = false (save costs)
